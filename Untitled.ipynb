{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03228bf-b7a7-4fc1-8457-9d2ddcc6d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "참고\n",
    "https://wikidocs.net/91051\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from urllib.request import urlretrieve, urlopen\n",
    "import gzip\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "import requests\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
    "zf = zipfile.ZipFile('glove.6B.zip')\n",
    "zf.extractall()\n",
    "zf.close()\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# def rating_transfer(rating):\n",
    "#     if rating > 5:\n",
    "#         rating = 1\n",
    "#     elif rating <= 5:\n",
    "#         rating = 0\n",
    "#     pn = rating\n",
    "#     return pn\n",
    "\n",
    "\n",
    "review_list = []\n",
    "rating_list = []\n",
    "base_url = \"https://www.imdb.com/\"\n",
    "key = \"\"\n",
    "\n",
    "\n",
    "# 수집하고 싶은 영화의 user riviews 페이지 url 붙여넣기\n",
    "url = \"https://www.imdb.com/title/tt6751668/reviews/?ref_=nv_sr_srsg_0\"\n",
    "\n",
    "# 수집하고 싶은 영화 리뷰 수 지정\n",
    "MAX_CNT = 30\n",
    "cnt = 0\n",
    "\n",
    "print(\"url = \", url)\n",
    "res = requests.get(url)\n",
    "res.encoding = \"utf-8\"\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "for item in soup.select(\".lister-list\"):\n",
    "    rating = item.select(\"span.rating-other-user-rating > span\")\n",
    "    if len(rating) == 2:\n",
    "        rating = rating[0].text\n",
    "    else:\n",
    "        rating = \"\"\n",
    "    review = item.select(\".text\")[0].text\n",
    "\n",
    "load_more = soup.select(\".load-more-data\")\n",
    "flag = True\n",
    "if len(load_more):\n",
    "    ajaxurl = load_more[0][\"data-ajaxurl\"]\n",
    "    base_url = base_url + ajaxurl + \"?ref_=undefined&paginationKey=\"\n",
    "    key = load_more[0][\"data-key\"]\n",
    "else:\n",
    "    flag = False\n",
    "\n",
    "while flag:\n",
    "    url = base_url + key\n",
    "    print(\"url = \", url)\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"utf-8\"\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "    for item in soup.select(\".lister-item-content\"):\n",
    "        rating = item.select(\"span.rating-other-user-rating > span\")\n",
    "        if len(rating) == 2:\n",
    "            rating = rating[0].text\n",
    "            review = item.select(\".text\")[0].text\n",
    "            pn = int(rating)\n",
    "            rating_list.append(pn)\n",
    "            review_list.append(review)\n",
    "            cnt = cnt + 1\n",
    "        else:\n",
    "            rating = \"\"\n",
    "        review = item.select(\".text\")[0].text\n",
    "\n",
    "        if cnt >= MAX_CNT:\n",
    "            break\n",
    "    if cnt >= MAX_CNT:\n",
    "        break\n",
    "    load_more = soup.select(\".load-more-data\")\n",
    "    if len(load_more):\n",
    "        key = load_more[0][\"data-key\"]\n",
    "    else:\n",
    "        flag = False\n",
    "\n",
    "df = pd.DataFrame(columns=[\"label\", \"review\"])\n",
    "df[\"review\"] = review_list\n",
    "df[\"label\"] = rating_list\n",
    "df.to_csv(\"IMDB_reviews.csv\")\n",
    "\n",
    "df.info()\n",
    "\n",
    "## 문장 토큰화\n",
    "# review 컬럼을 반복하여 sent_tokenize()를 이용해 문장 단위로 분리\n",
    "sentences = []\n",
    "for s in df[\"review\"]:\n",
    "    sentences.extend(sent_tokenize(s))\n",
    "print(\"분리된 문장 개수:\", len(sentences))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#문장 단위로 분리된 데이터를 확인\n",
    "for i in range(len(sentences)):\n",
    "    print('{}번 째 문장: {}\\n'.format(i+1, sentences[i]))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##단어 토큰화\n",
    "# 문장 단위로 분리된 데이터를 'sentences'열에 대해서 text_to_word_sequence()를 적용한 'tokenized_sentences' 열을 새로 만듭니다.\n",
    "df[\"tokenized_sentences\"] = df[\"review\"].apply(text_to_word_sequence)\n",
    "\n",
    "# 사전 훈련된 GloVe\n",
    "glove_dict = dict()\n",
    "f = open(\"glove.6B.100d.txt\", encoding=\"utf8\")  # 100차원의 GloVe 벡터를 사용\n",
    "\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    word_vector_arr = np.asarray(\n",
    "        word_vector[1:], dtype=\"float32\"\n",
    "    )  # 100개의 값을 가지는 array로 변환\n",
    "    glove_dict[word] = word_vector_arr\n",
    "f.close()\n",
    "\n",
    "#  GloVe 벡터의 차원은 100. 100차원의 영벡터를 만든다.\n",
    "embedding_dim = 100\n",
    "zero_vector = np.zeros(embedding_dim)\n",
    "\n",
    "# 단어 벡터의 평균으로부터 문장 벡터를 얻는다.\n",
    "def calculate_sentence_vector(sentence):\n",
    "    if len(sentence) != 0:\n",
    "        return sum([glove_dict.get(word, zero_vector) for word in sentence]) / len(\n",
    "            sentence\n",
    "        )\n",
    "    else:\n",
    "        return zero_vector\n",
    "\n",
    "\n",
    "# 각 문장에 대해서 문장 벡터를 반환\n",
    "def sentences_to_vectors(sentences):\n",
    "    return [calculate_sentence_vector(sentence) for sentence in sentences]\n",
    "\n",
    "\n",
    "# 모든 문장에 대해서 문장 벡터를 만든다.\n",
    "df[\"SentenceEmbedding\"] = df[\"tokenized_sentences\"].apply(sentences_to_vectors)\n",
    "\n",
    "# 문장 벡터들 간의 코사인 유사도를 구한 유사도 행렬을 만든다.\n",
    "def similarity_matrix(sentence_embedding):\n",
    "    sim_mat = np.zeros([len(sentence_embedding), len(sentence_embedding)])\n",
    "    for i in range(len(sentence_embedding)):\n",
    "        for j in range(len(sentence_embedding)):\n",
    "            sim_mat[i][j] = cosine_similarity(\n",
    "                sentence_embedding[i].reshape(1, embedding_dim),\n",
    "                sentence_embedding[j].reshape(1, embedding_dim),\n",
    "            )[0, 0]\n",
    "    return sim_mat\n",
    "\n",
    "\n",
    "# 이 결과를 저장한 'SimMatrix'열을 만든다.\n",
    "df[\"SimMatrix\"] = df[\"SentenceEmbedding\"].apply(similarity_matrix)\n",
    "df[\"SimMatrix\"]\n",
    "\n",
    "# 유사도 행렬로부터 그래프를 그린다.\n",
    "def draw_graphs(sim_matrix):\n",
    "  nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  pos = nx.spring_layout(nx_graph)\n",
    "  nx.draw(nx_graph, with_labels=True, font_weight='bold')\n",
    "  nx.draw_networkx_edge_labels(nx_graph,pos,font_color='red')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "draw_graphs(df['SimMatrix'][1])\n",
    "\n",
    "\n",
    "\n",
    "# 페이지랭크 알고리즘의 입력으로 사용하여 각 문장의 점수를 구한다.\n",
    "def calculate_score(sim_matrix):\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    return scores\n",
    "\n",
    "\n",
    "df[\"score\"] = df[\"SimMatrix\"].apply(calculate_score)\n",
    "print(df[[\"SimMatrix\", \"score\"]])\n",
    "\n",
    "# 이 점수가 가장 높은 문장들을 상위 n개 선택하여 이 문서의 요약문으로 삼을 것이다. 점수가 가장 높은 상위 3개의 문장을 선택하는 함수를 만든다. 점수에 따라서 정렬 후에 상위 3개 문장만을 반환\n",
    "def ranked_sentences(sentences, scores, n=3):\n",
    "    top_scores = sorted(\n",
    "        ((scores[i], s) for i, s in enumerate(sentences)),reverse=True)\n",
    "    top_n_sentences = [sentence for score, sentence in top_scores[:n]]\n",
    "    return \" \".join(top_n_sentences)\n",
    "\n",
    "\n",
    "# 'ranked_sentences' 함수를 적용하여 'summary'열을 만든다.\n",
    "df[\"summary\"] = df.apply(\n",
    "    lambda x: ranked_sentences(x.sentences, x.score),axis=1)\n",
    "\n",
    "# 결과 확인\n",
    "# df[[\"review\", \"summary\"]]\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "  print(i+1,'번 문서')\n",
    "  print('원문 :',df.loc[i].article_text)\n",
    "  print('')\n",
    "  print('요약 :',df.loc[i].summary)\n",
    "  print('')\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#단어 단위로 분리된 데이터를 확인\n",
    "for i in range(len(words)):\n",
    "    print('{}번 째 단어: {}\\n'.format(i+1, words[i]))\n",
    "\"\"\"\n",
    "\n",
    "##불용어 제거\n",
    "# 불용어 목록을 불러온 후, 단어 단위로 분리된 데이터를 반복하여 불용어 제거\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# words의 단어 길이가 1이하인 단어 제거\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "# words의 단어가 숫자인 단어 제거\n",
    "words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "# words의 단어가 영어가 아닌 단어 제거\n",
    "words = [word for word in words if word.isalpha()]\n",
    "\n",
    "# words의 단어를 소문자로 변환\n",
    "words = [word.lower() for word in words]\n",
    "\n",
    "# words의 단어의 품사가 명사, 형용사, 부사, 동사인 단어만 추출\n",
    "tagged = nltk.pos_tag(words)\n",
    "words = [\n",
    "    word\n",
    "    for word, tag in tagged\n",
    "    if tag\n",
    "    in [\n",
    "        \"NN\",\n",
    "        \"NNS\",\n",
    "        \"NNP\",\n",
    "        \"NNPS\",\n",
    "        \"JJ\",\n",
    "        \"JJR\",\n",
    "        \"JJS\",\n",
    "        \"RB\",\n",
    "        \"RBR\",\n",
    "        \"RBS\",\n",
    "        \"VB\",\n",
    "        \"VBD\",\n",
    "        \"VBG\",\n",
    "        \"VBN\",\n",
    "        \"VBP\",\n",
    "        \"VBZ\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "##불용어 제거 후 단어 개수 확인\n",
    "print(\"불용어 제거 후 단어 개수:\", len(words))\n",
    "\n",
    "\"\"\"\n",
    "##불용어 제거 후 단어 확인\n",
    "for i in range(len(words)):\n",
    "    print('{}번 째 단어: {}\\n'.format(i+1, words[i]))\n",
    "\"\"\"\n",
    "\n",
    "##표제어 추출\n",
    "# 단어 단위로 분리된 데이터를 반복하여 표제어 추출\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "words = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "##표제어 추출 후 단어 개수 확인\n",
    "print(\"표제어 추출 후 단어 개수:\", len(words))\n",
    "\n",
    "\"\"\"\n",
    "##표제어 추출 후 단어 확인\n",
    "for i in range(len(words)):\n",
    "    print('{}번 째 단어: {}\\n'.format(i+1, words[i]))\n",
    "\"\"\"\n",
    "\n",
    "##단어 빈도수 확인\n",
    "# 단어 단위로 분리된 데이터를 반복하여 단어 빈도수 확인\n",
    "word_count = Counter(words)\n",
    "print(\"단어 빈도수:\", word_count)\n",
    "\n",
    "\n",
    "#######textrank 사용해서 수정!\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a30c2-01cc-424f-a3b6-805c961e7ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
